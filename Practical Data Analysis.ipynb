{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69520795-412f-4f72-ab88-b88cc40c4a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0634a-cd7a-4176-8b10-8c7f6ef9d20f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407ca52-e850-4f74-890d-c6689520361b",
   "metadata": {},
   "source": [
    "One could think that the main object of data analysis is a set of observations $ \\{x_i\\} $ and an existing relationship between these measurements. These assumptions are mainly a belief that there is a deeper reality besides the measurement. For us, this relation is materialized by the probability distribution $P$.\n",
    "\n",
    "The probability of a measument $x$ given an a hypothesis $\\theta$ is $$P(x|\\theta).$$ There are two main methods of working in this space.\n",
    "\n",
    "* Parametric methods: We know the probability distribuition, or at least we think so, then we can simulate a sample;\n",
    "\n",
    "* Unparametric methods: We do not know the probability distribuition, therefore, we try to discover it via our observations set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4eb436-ced1-408b-bad4-f5426bbb00fe",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9c637-01e0-4482-a973-cf379884d025",
   "metadata": {},
   "source": [
    "We begin by describing our dataset. For instance, say we have a set observation of the position of galaxies in our sky. If we start by applying statistics to their actual position, we would be inclined to a specific probability distribution $P$. Moreover, if we decide to \"clump\" our galaxies into particular pixels, we would find a whole new distribution.\n",
    "\n",
    "Furthermore, it is maximal that we correctly describe our data statistically. Let us say we have two different distributions:\n",
    "\n",
    "$$.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\; \\{x_1\\}$$\n",
    "\n",
    "$$........... \\{x_2\\}$$\n",
    "\n",
    "If we were to calculate the mean value of both, we would get the same value $\\overline{x}$, but it is clear that these dataset are very diferent. Therefore, we must calculate other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99407251-0f8f-4300-8ce0-569646d82076",
   "metadata": {},
   "source": [
    "## Computational methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecae47-bb07-4e01-99a1-08fb1072e9d9",
   "metadata": {},
   "source": [
    "We shall use computational methods to generate a sample $\\{x_i\\}^N$ and calculate the value of $P(x)$ for every generated point. The first problem we will encounter is the fact that computers are intrinsically integer finite machines. \n",
    "\n",
    "The solution to our integrity problem lies in floating-point numbers. We shall give a tuple of an integer, and the position of the point. This is the definition of ***float values***!  (づ￣ ³￣)づ\n",
    "\n",
    "Now, to solve the finiteness problem, we implement interpolation. Intead of calculating every value $P(x)$, we will use two calculated values for $(x_1;P(x_1))$ and $(x_2; P(x_2))$ to find a third without the explicit calculation $P(x)$. As an example, we can find the value of $P(x)$ via a linear interpolation \n",
    "$$ \n",
    "\\overline{P}(x) = P_1 + (P_2 - P_1)\\frac{(x-x_1)}{(x_2-x_1)} \\;;\\; x_1<x<x_2\n",
    "$$\n",
    "\n",
    "Then, our fucntion will be described by\n",
    "\n",
    "$$\n",
    "P(x) = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        P(x_1) & \\mbox{if } \\ x = x_1 \\\\\n",
    "        \\overline{P}(x) & \\mbox{if } \\ x = x \\\\\n",
    "        P(x_2) & \\mbox{if } \\ x=x_2 \\\\\n",
    "        \\vdots\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "We could use interpolation of a greater order. However, these methods are much more complex and require descriptions of the differential of $\\overline{P}(x)$  to impose continuity of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2778c65-7cda-4451-aa00-7d3b4f918bb1",
   "metadata": {},
   "source": [
    "## Random varibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef4ff-d331-46bd-802d-15f9418fafa3",
   "metadata": {},
   "source": [
    "Given a probability distribution $P(x)$, we define\n",
    "\n",
    "$$\n",
    "C(x) = \\int_a^b P(x) dx.\n",
    "$$\n",
    "\n",
    "Furthermore, let us say $x$ is a random variable, with an expected value\n",
    "\n",
    "$$\n",
    "\\langle x \\rangle = \\int P(x) x dx.\n",
    "$$\n",
    "\n",
    "Therefore, if $x$ is a random variable, then $C(x)$ will also be a random variable. Moreover, we might ask what is the probability of $C(x)$ be equal a certain value. This will give us a probability distribuition for $C$, \n",
    "\n",
    "$$\n",
    "P(C = C_0) = \\int P(x) \\delta(C(x) - C_0(x)) dx.\n",
    "$$\n",
    "\n",
    "This integral is easy to solve. By the definition of $C(x)$, we know $\\partial_x C(x) = P(x)$. Since the probability distribution is a positive definite function, $C(x)$ is a monotonic function, i.e., $C$ is an inversible function. Now, because $C$ is inversible, the argument of the Dirac's delta function is defined on only one point $C(x_0) = C_0$. Replacing $C_0(x)$ by $C(x_0)$ on the integral, and using the fact that\n",
    "\n",
    "$$\n",
    "\\delta(f(x)) = \\frac{\\delta(x-x_0)}{\\left|\\frac{\\partial f(x_0)}{\\partial x}\\right|},\n",
    "$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\n",
    "P(C = C_0) = \\frac{P(x_0)}{\\left|\\frac{\\partial C(x_0)}{\\partial x}\\right|} = 1.\n",
    "$$\n",
    "\n",
    "This confirms that our distribution gives the equal probability for every possible value of our dataset.\n",
    "\n",
    "Now we can calculate $C(x)$ using a set $\\{x_i , C_i\\}$. Not only $C(x)$, we can also calculate $\\widetilde{X}(C)$, using the fact that $C(x)$ is inversible. We are doing all this to get a random sample. Since a computer can give an algorithm that takes a random sample from a uniform distribution, we do not need to know how to choose values for the calculation of $P(x)$. Therefore, given an uniform distribution $U(a, b) \\rightarrow \\{U_i\\}$, we can generate a random sample ${X_i} = \\widetilde{X}(U_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fe9e4-f6e0-4e55-a57d-d5ec307d006a",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237fb0e-dfa8-4dba-b384-184f4d6e0b40",
   "metadata": {},
   "source": [
    "*For further reading, see* ***https://sites.warnercnr.colostate.edu/gwhite/wp-content/uploads/sites/73/2017/04/BinomialLikelihood.pdf*** \n",
    "\n",
    "Say we have a certain event with $P$ probability of happening. With $n$ occurencies, this particular event happended $r$ times, the likelihood will be given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p|n, r) =\\binom{n}{r} p^r (1-p)^{n-r}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8633c1-70b2-418e-afac-95ae656e2ebd",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "***https://www.youtube.com/watch?v=XjFYc9ynagk&ab_channel=SandroDiasPintoVitenti***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237414bd-977d-4302-bda4-95ab7ba3ebaa",
   "metadata": {},
   "source": [
    "## Describing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090744c-d9cf-4bfa-add5-fd9164d43974",
   "metadata": {},
   "source": [
    "As we are working with *hard sciences*, most of our data will be quantitative. This is a reflection of our main objective, which is to build probability distribution from basic theoretical principles.\n",
    "\n",
    "We can divide quantitative data into two main fields: discrete and continuous.\n",
    "\n",
    "* Discrete: Probabilities are countable;\n",
    "\n",
    "* Continuous: Probabilities are uncountable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09171a-1559-4fd0-99ad-f72eefbbdcba",
   "metadata": {},
   "source": [
    "## Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a80d4-33d2-4b2e-b81a-ea4e7d65eaea",
   "metadata": {},
   "source": [
    "One is capable of building a map between discretes and continuos data with  histograms. For instance, given a colection of measurements of mass (*Data*)\n",
    "\n",
    "$$\n",
    "\\{m_{i}\\}_{i\\in\\left[1,N\\right]} = D\n",
    "$$\n",
    "\n",
    "we shall build a set knots\n",
    "\n",
    "$$\n",
    "\\{m_i^k\\}_{i\\in\\left[1, M+1\\right]},\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b_i = Count\\{m_i^k \\leq m_i < m_{i+1}^k\\}.\n",
    "$$\n",
    "\n",
    "The set of $\\{b_i\\}_{i\\in\\left[1,M\\right]}$ is a discrete set. Furthermore, this set has a reduced dimensionality. This could mean we are losing information, but there are ways for us to measure how good our set of *bins* is.\n",
    "\n",
    "We call $b_i$ **statistics**. It is a function of our data, $b_i\\left(D\\right)$. Moreover, we will see differents examples of **statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc293a0a-497c-4ca3-8516-cfa0eed5d624",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a7bc-024f-4448-8e80-38154d158c6e",
   "metadata": {},
   "source": [
    "One possible sumary we can derive from our data is the *arithmetic mean*. It is defined as\n",
    "\n",
    "$$\n",
    "\\overline{X}(D) \\equiv \\sum_{i=1}^N \\frac{X_i}{N}\\;.\n",
    "$$\n",
    "\n",
    "Additionaly, we can calculate the *mean* from function with\n",
    "\n",
    "$$\n",
    "\\overline{f}(D) \\equiv \\sum_{i=1}^N \\frac{f(x_i)}{N}\\;.\n",
    "$$\n",
    "\n",
    "There are different *mean values*. For instance, we can summarize our data into the *geometric mean* with\n",
    "\n",
    "$$\n",
    "\\overline{X}^g (D)\\equiv \\prod_{i=1}^N \\left[X_i\\right]^{\\frac{1}{N}},\n",
    "$$\n",
    "\n",
    "which could usefully be written as\n",
    "\n",
    "$$\n",
    "\\overline{X}^g (D) = e^{\\sum_{i=1}^N\\ln X_i/N}.\n",
    "$$\n",
    "\n",
    "There are two other *mean* values. *Harmonic mean* and *root mean square*, respectively given as\n",
    "\n",
    "$$\n",
    "H \\equiv \\frac{N}{\\sum_{i=1}^N \\frac{1}{X_i}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "RMS \\equiv \\sqrt{\\frac{\\sum_{i=1}^N X_i^2}{N}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63b904-d444-425a-97bd-e465bbafac50",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff36722-d868-45f9-809f-2bef3c3cc47c",
   "metadata": {},
   "source": [
    "Given three measurements,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf89e900-2ab9-4840-a3a3-0d9aef02a1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39me\n\u001b[1;32m      3\u001b[0m x3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x1, x2, x3])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x1 = 1\n",
    "x2 = np.e\n",
    "x3 = np.exp(5)\n",
    "arr = np.array([x1, x2, x3])\n",
    "\n",
    "print(f'x1: {x1}\\nx2: {x2}\\nx3: {x3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f0133-8501-4aa5-b64f-566e1b3a2bc2",
   "metadata": {},
   "source": [
    "if we calculate the *arithmetic mean*, $\\overline{X}$, we will find a value dominated by the biggest measurement, $e^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3bde5-224e-438a-b9aa-ad81cca15783",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = np.mean(arr)\n",
    "\n",
    "print(f'x_bar: {x_bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79081a-f0f4-4d24-943d-ace843bd51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr, len(arr) * [1], 'x')\n",
    "plt.plot(x_bar, 1, 'o', label = 'x_bar')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72aba9-c200-4bfd-95a1-242ce446f58d",
   "metadata": {},
   "source": [
    "As we can see, the value of $\\overline{X}$ is masked by the dispersion of this distribution. However, if we calculate $\\overline{X}^g$, we find a reasonable description of our measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9c8f7-c95c-48a9-83bc-59771109853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7534c-9811-43c3-ae70-958207a7223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_barg = gmean(arr)\n",
    "\n",
    "print(f'x_barg: {x_barg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251918a6-075b-42fb-98ce-7ef406f3ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr, len(arr) * [1], 'x')\n",
    "plt.plot(x_barg, 1, 'o', label = 'x_barg')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283df35-ee4b-40c9-bef1-8e0d65d9505a",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28229333-f408-4102-944c-19cf2622ebfe",
   "metadata": {},
   "source": [
    "The *median* is the mean value of the midle terms from the ordenation of out sample.\n",
    "\n",
    "Let us say that \n",
    "$$\n",
    "Y_i = X_{\\mathcal O(i)},\\; where \\; Y_{i} \\leq Y_{i+1}.\n",
    "$$ \n",
    "\n",
    "The *median* is $Y_{|\\underline{N/2}|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1e828-12bc-4e17-a6b5-7a88747cacaf",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b7796-a867-4d32-a8d2-bebb175d21bf",
   "metadata": {},
   "source": [
    "*Mode* is the value with highest repetition from a sample.\n",
    "\n",
    "$$\n",
    "L_i = X_i,\\; where \\; Count\\{X_i\\}>Count\\{X_j\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4c296-82a2-4486-b480-81a3175d0ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Robust Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7e8c0-1adf-4e71-85e9-9044f975f3bc",
   "metadata": {},
   "source": [
    "**Robust statistics** is statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. [[Wiki]](https://en.wikipedia.org/wiki/Robust_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceead0f5-a65d-41ef-a4e0-890e40e9fef5",
   "metadata": {},
   "source": [
    "### Mean Absolute Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b4e82-bea8-4b10-8588-62d0172dc481",
   "metadata": {},
   "source": [
    "The mean absolute deviation of a dataset is the average distance between each data point and the mean. It gives us an idea about the variability in a dataset.[[Khan]](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/other-measures-of-spread/a/mean-absolute-deviation-mad-review)\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N\\frac{\\left|X_i - \\overline{X}\\right|}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288549be-1343-455d-adec-a4552d9b1aa0",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adf92d-f337-4a63-8a31-2e4080a6cd1a",
   "metadata": {},
   "source": [
    "Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\n",
    "\n",
    "$$\n",
    "V(X) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^2 = \\overline{X^2} - \\overline{X}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d52335-afef-4db2-a40e-2ebb5a44af61",
   "metadata": {},
   "source": [
    "### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c333d-fc1b-47ac-a174-75c0b1eb5cad",
   "metadata": {},
   "source": [
    "The standard deviation is a measure of the amount of variation or dispersion of a set of values.\n",
    "\n",
    "$$\n",
    "\\sigma \\equiv \\sqrt{V(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fbb3c-b492-4d21-8bbf-4ab7eff9013e",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c1258-a826-45ee-8272-a81aabb58a38",
   "metadata": {},
   "source": [
    "Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n",
    "\n",
    "$$\n",
    "\\gamma \\equiv \\frac{1}{N \\sigma^3}\\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904901d-ab0d-4ecb-8dfc-70d16a891ebd",
   "metadata": {},
   "source": [
    "### Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221828-fbfb-4f39-a3f7-1c6d9a38f129",
   "metadata": {},
   "source": [
    "Kurtosis (from Greek: κυρτός, kyrtos or kurtos, meaning \"curved, arching\") is a measure of the \"tailedness\" of the probability distribution.\n",
    "\n",
    "$$\n",
    "K \\equiv \\frac{1}{N \\sigma^4}\\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^4 - 3\n",
    "$$\n",
    "\n",
    "This extremely weird $-3$ is a normalization factor so that the *kurtosis* of the *Gaussian Distribuition* is **zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2ad7f-afce-448e-b544-70e6ecc1b713",
   "metadata": {},
   "source": [
    "### Central Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a3c7e-9b3d-44d6-bcd5-221e94c3e60e",
   "metadata": {},
   "source": [
    "Given a estimator\n",
    "\n",
    "$$\n",
    "M_r \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^r = \\overline{\\left(X_i - \\overline{X}\\right)^r},\n",
    "$$\n",
    "\n",
    "called *central moment*, we are could describe a new summary\n",
    "\n",
    "$$\n",
    "\\{\\overline{X}, M_2, M_3, ..., M_N\\}.\n",
    "$$\n",
    "\n",
    "This new summary is capable of correctly define all our data's information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a073801-b67c-4705-95ab-0088d1da3314",
   "metadata": {},
   "source": [
    "## Multivariable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbbf8a-7745-4f73-b5b2-e93e716fae8a",
   "metadata": {},
   "source": [
    "Sometimes our measurements give rise to multiple information, *i.e.*,\n",
    "\n",
    "$$\n",
    "D = \\{\\left(X_i, Y_i)\\right)\\}_{i \\in \\left[1, N\\right]}.\n",
    "$$\n",
    "\n",
    "In this case, we need to apply different **statistics** to summarize this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c6a05-63dc-4962-8ab6-90e512565599",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d870116-7ca6-4265-adbb-a9c48fcb7005",
   "metadata": {
    "tags": []
   },
   "source": [
    "Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values (that is, the variables tend to show similar behavior), the covariance is positive. \n",
    "\n",
    "In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (that is, the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. [[Wiki]](https://en.wikipedia.org/wiki/Covariance)\n",
    "\n",
    "$$\n",
    "Cov\\left(X,Y\\right) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)\\left(Y_i - \\overline{Y}\\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058bfa4-731b-427d-af82-4e79de9e6da8",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab140-a4c0-4156-9b21-e8088ecc94de",
   "metadata": {
    "tags": []
   },
   "source": [
    "Correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it normally refers to the degree to which a pair of variables are linearly related.[[Wiki]](https://en.wikipedia.org/wiki/Correlation)\n",
    "\n",
    "Correlation is a normalized way of writing the covariance.\n",
    "\n",
    "$$\n",
    "Cor\\left(X, Y\\right) = \\rho_{XY} \\equiv \\frac{Cov\\left(X, Y\\right)}{\\sigma_X \\sigma_Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a24a05-0fb4-419b-a023-2e6f0d987289",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412832e8-ae0a-4a73-b7f6-bea391c2f083",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804d26a-37d4-41ac-8d94-ed9790cae0b2",
   "metadata": {},
   "source": [
    "Let's say we have four identical coins. They have two sides - heads or tails - with an equal probability of arising. Taking a deeper look into the probabilities, we can build our distribution. They are the following\n",
    "\n",
    "* Four tails: $\\frac{1}{16}$\n",
    "* Three tails: $\\frac{4}{16}$\n",
    "* Two tails: $\\frac{6}{16}$\n",
    "* One tail: $\\frac{4}{16}$\n",
    "* Four heads: $\\frac{1}{16}$\n",
    "\n",
    "Therefore, it is expected that if we toss our set of coins sixteen times, we should see this distribution. Let us create a simple example with a numerical analogy. In the next kernel, we will create an array with zeroes (tails) and ones (heads). You will probably see that is very unlikely our distribution matches with our random experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11084a08-2c9a-4d83-9596-977031eae80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creates a numpy array of shape (16,4), filled with random integers between 0 (inclusive) and 2 (exclusive)\n",
    "rand_array = np.random.randint(0, 2, (16,4)) \n",
    "rand_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cabde-d4f5-4397-8921-99f0e0b4b630",
   "metadata": {},
   "source": [
    "## Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f3f30-e952-4b53-9689-887242863af2",
   "metadata": {},
   "source": [
    "The **Law of Large Numbers** is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.[[Wiki]](https://en.wikipedia.org/wiki/Law_of_large_numbers) Furthermore, as our sample grows, the empirical probability distribution narrows down to the theoretical distribution, *i.e.*\n",
    "\n",
    "$$\n",
    "\\frac{n_i}{N} \\rightarrow P_i \\;| N\\rightarrow \\infty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8285b4-7997-4607-b6ef-2916d66c6979",
   "metadata": {},
   "source": [
    "## Expected Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a56b1-a5c8-4c6b-bfa2-8e487de41c78",
   "metadata": {},
   "source": [
    "**Expected value** (also called expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable.[[Wiki]](https://en.wikipedia.org/wiki/Expected_value)\n",
    "\n",
    "$$\n",
    "\\langle r\\rangle  = \\sum_r r P(r)\\; .\n",
    "$$\n",
    "\n",
    "Moreover, one can find the expected value of a function with:\n",
    "\n",
    "$$\n",
    "\\langle f\\rangle  = \\sum_r f(r) P(r)\\; .\n",
    "$$\n",
    "\n",
    "It is easy to show that\n",
    "\n",
    "$$\n",
    "\\langle f+g\\rangle  = \\sum_r \\left(f(r)+g(r)\\right) P(r) = \\langle f\\rangle  + \\langle g\\rangle \\;,\n",
    "$$\n",
    "\n",
    "and if $P(r,s) = P_1(r)P_2(s)$, $f(r,s) = f(r)$ and $g(r,s) = g(s)$\n",
    "\n",
    "$$\n",
    "\\langle fg\\rangle  = \\sum_{r,s} f(r)g(s) P_1(r)P_2(s) = \\langle f\\rangle \\langle g\\rangle \\;.\n",
    "$$\n",
    "\n",
    "Let us roll back to our last example. What is the expected value of the number of tails?? Using the probability distribution we get:\n",
    "\n",
    "$$\n",
    "\\langle r\\rangle  = 0 \\times \\frac{1}{16} + 1 \\times \\frac{4}{16} + 2 \\times \\frac{6}{16} + 3 \\times \\frac{4}{16} + 4 \\times \\frac{1}{16} = \\frac{32}{16} = 2   \n",
    "$$\n",
    "\n",
    "The most probable number of tails is two, and this seems like a reasonable answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf93c3-6d04-42ed-9969-df7099c2164f",
   "metadata": {},
   "source": [
    "## Probability Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c4e93-bfa8-44ed-9ce0-11b9b9c384a2",
   "metadata": {},
   "source": [
    "What if our random varible takes values from the $\\mathcal{R}^1$? For instance, if the measurement of a certain ruller is $11 cm$, the probability of this value is\n",
    "\n",
    "$$\n",
    "\\frac{n_{11cm}}{N} = P_{11cm}\\;.\n",
    "$$\n",
    "\n",
    "However, since $N = \\infty$, the probability for every possible value in the number line is $zero$. To counter this theoretical *conundrum*, we will create sections of our possible values. Given a sufficiently small $\\varepsilon$, our new probability shall be written as\n",
    "\n",
    "$$\n",
    "\\frac{n\\left[(11cm - \\varepsilon, 11cm + \\varepsilon)\\right]}{N} = P_{interval}\\;.\n",
    "$$\n",
    "\n",
    "With this in mind, we will generaly redefine probability on a said interval $P[(a,b)] \\equiv P(x)$.  Now we define $P'(x) = p(x)$, *i.e.*\n",
    "\n",
    "$$\n",
    "P(x) = \\int p(x) dx\\;,\n",
    "$$\n",
    "where $p(x)$ is the **Probability density**.\n",
    "\n",
    "\n",
    "The **Probability density**, or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be close to that sample. In other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample. [[Wiki]](https://en.wikipedia.org/wiki/Probability_density_function)\n",
    "\n",
    "As well as we ahve done for discrete values, we can define expected values with the **probability density**. \n",
    "\n",
    "$$\n",
    "\\langle x\\rangle  = \\int_{-\\infty}{^\\infty} x p(x) dx\\;;\\\\\n",
    "\\langle f\\rangle  = \\int_{-\\infty}{^\\infty} f(x) p(x) dx\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff949e-927d-4e09-b3ad-cf7f9e37c1b9",
   "metadata": {},
   "source": [
    "## Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20674953-3d4b-4798-91c2-5a6796d2dd72",
   "metadata": {},
   "source": [
    "Given a experiment, with two -and only two- possible outcomes $\\{A, B\\}$, the sum of probabilities $P_A$ and $P_B$ will be equal to $one$. Additionally, with $N$ events, the probability of only $p$ measurements being $A$ is\n",
    "$$\n",
    "P(p) = P_A^{p}P_B^{N-p}\\binom{N}{p}\\;.\n",
    "$$\n",
    "\n",
    "It is easy to check the normalization of our probability. If we sum the above expression for every possible $p$ we find\n",
    "$$\n",
    "\\sum_p^N P(p) = \\sum_p^N P_A^{p}P_B^{N-p}\\binom{N}{p}\\;.\n",
    "$$\n",
    "Where the right side is know as the binomial expansion [[Wiki]](https://en.wikipedia.org/wiki/Binomial_theorem), therefore\n",
    "$$\n",
    "\\sum_p^N P(p) = (P_A + P_B)^N = 1\\;.\n",
    "$$\n",
    "\n",
    "Moreover, we can calculate the expected value with a simple trick. Since $\\langle p\\rangle  = \\sum_{p=0}^N p P(p)$, we will replace the probability of $p$ by its binomial form. The trick is to notice the value of $p$ in the expoent and as a factor of the sum. Then we shall write the binomial form times the value $p$ as a derivative.\n",
    "\n",
    "$$\n",
    "\\langle p\\rangle  = \\sum_{p=0}^N p P_A^{p}P_B^{N-p}\\binom{N}{p} = P_a \\left[ \\frac{\\partial}{\\partial P_A} \\left( \\sum_{p=0}^N P_A^p P_B^{N-p} \\binom{N}{p} \\right) \\right] = P_A \\frac{\\partial}{\\partial P_A}\\left(P_A + P_B\\right)^N\\;,\\\\\\langle p\\rangle  = N P_A\n",
    "$$\n",
    "\n",
    "Furthermore, one can find the variance of the distribuition. We know the variance of a value is $\\langle \\alpha^2\\rangle  - \\langle \\alpha\\rangle ^2$. Using the derivation trick twice on the binomial form of $P(p)$ we find\n",
    "\n",
    "$$\n",
    "\\langle p (p -1)\\rangle  = P_A^2 \\frac{\\partial}{\\partial P_A}\\left(P_A + P_B\\right)^N = P_A^2N(N-1)\\;.\n",
    "$$\n",
    "\n",
    "Using the sum property for the expected value, it is clear that $\\langle p^2\\rangle  = P_A^2N(N-1) + \\langle p\\rangle $. Therefore, the variance will be\n",
    "\n",
    "$$\n",
    "\\langle p^2\\rangle  - \\langle p\\rangle ^2 = N P_A P_B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8774d-fc7c-4fad-94a6-1ca9f9728022",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922b52d-e6cc-44ff-a60a-f118138b8454",
   "metadata": {},
   "source": [
    "## Poisson's Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24621e42-efc9-443d-a866-ffa473b0bec9",
   "metadata": {},
   "source": [
    "Suppose we have an average of $\\lambda$ events that will happen in a given continuous interval $(a,b)$. Additionally, we subdivide our interval into $n$ equally sparsed sections. We begin by questioning what is the probability of $r$ events occurring in one particular section $n_x$.\n",
    "$$\n",
    "_a(- - -\\;r- - - - - - - - - -)_b\n",
    "$$\n",
    "\n",
    "Based on the **Binomial distribution**, we shall fix the expected value $\\langle r\\rangle  = n P = \\lambda$ as $n$ tends to $\\infty$. With $P_B = 1-P_A$ and $P_A = \\lambda / n$, we find:\n",
    "\n",
    "$$\n",
    "P(r) = \\left(\\frac{\\lambda}{n}\\right)^r \\left(1 - \\frac{\\lambda}{n}\\right)^{n-r} \\binom{n}{r}\\;.\n",
    "$$\n",
    "\n",
    "Moreover, using **Stirling approximation** we can reduce the above expression to:\n",
    "\n",
    "$$\n",
    "P(r) = e^{-\\lambda}\\frac{\\lambda^r}{r!}\\;.\n",
    "$$\n",
    "\n",
    "One can also prove that a poissonic distribution has variance of $\\lambda$ and a standard deviation of $\\sigma_r = \\sqrt{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36fb31-b73b-492d-87a9-148b479d1e85",
   "metadata": {},
   "source": [
    "## Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c69fe-0886-4c73-86dd-385e7be85980",
   "metadata": {},
   "source": [
    "The Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}}\\;.\n",
    "$$\n",
    "\n",
    "With a couple of tricks we can show that:\n",
    "\n",
    "- The distribution is normalized, *i.e.*, $\\int_{-\\infty}^{\\infty}f(x)dx = \\int_{-\\infty}^{\\infty}dx\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}} = 1\\;;$\n",
    "- Its expected value $\\langle x\\rangle  = \\int_{-\\infty}^{\\infty}x \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}}dx  = \\mu \\;;$\n",
    "- Its variance is $\\langle (x-\\mu)^2\\rangle  = \\int_{-\\infty}^{\\infty}dx (x-\\mu)^2 \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}} = \\sigma^2 \\;.$\n",
    "\n",
    "***P.S.:*** *This derivation would be a pain to do entirely in $\\LaTeX$. There is a Xournal File in the repository with the complete analysis.* \n",
    "\n",
    "Additionally, we shall formally call $1\\sigma$ the calculated area bellow the normal distribution between the interval $(\\mu-\\sigma, \\mu+\\sigma)$. This is an important value because it does not depend on $\\sigma$ and $\\mu$! By calculating the integral on a said interval, with a simple variable change we find:\n",
    "\n",
    "$$\n",
    "1\\sigma = \\int_1^1 \\frac{e^{-\\frac{y^2}{2}}}{\\sqrt{2\\pi}}dy \\approx 0,6827...\n",
    "$$\n",
    "\n",
    "It is common to say that a certain event has a probability $1\\sigma$ of happening. This is widely used as a range of confiability. As we get more $\\sigma$, there will be a greater confiability of a certain expected value:\n",
    "\n",
    "- $1\\sigma = 0,6827...$\n",
    "- $1,645\\sigma = 0,9$\n",
    "- $2\\sigma = 0,9545...$\n",
    "- $3\\sigma = 0,9973...$\n",
    "\n",
    "Bellow you can see a plot of this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa4df9-67b3-4eb0-8069-e42b640854b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-2, 4, 39)\n",
    "mu = 1\n",
    "sigma = 1\n",
    "\n",
    "plt.plot(t, fs.gaussian_distribution(t, mu, sigma), c = 'r')\n",
    "\n",
    "plt.fill_between(\n",
    "        x= t, \n",
    "        y1= fs.gaussian_distribution(t, mu, sigma), \n",
    "        where= (mu - sigma < t)&(t < mu + sigma),\n",
    "        color= \"b\",\n",
    "        alpha= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb528-a1ba-4d54-a977-e719379e8485",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96753bd-e888-4e87-ae50-d751f31ca8db",
   "metadata": {},
   "source": [
    "Given a vector $\\vec{x} \\in \\mathcal{M}^n$, the probability of measuring a certain value for $\\vec{x}$ is:\n",
    "\n",
    "$$\n",
    "P(\\vec{x}) = \\frac{\\exp\\left[{\\frac{- (\\vec{x} - \\vec{\\mu})^T C^{-1} (\\vec{x}-\\vec{\\mu})}{2}}\\right]}{\\left(\\sqrt{2\\pi}\\right)^n \\sqrt{det\\;C}}\\;.\n",
    "$$\n",
    "\n",
    "Where $C$ is a defined-positive symmetric matrix, *i.e.*, $\\frac{\\vec{v}^T C \\vec{v}}{|\\vec{v}|} > 0$ for every $\\vec{v} \\in \\{v\\}$ (a possible basis for $\\mathcal{M}^n$). Furthermore, the *eigen-value* of $C$ is always:\n",
    "\n",
    "$$\n",
    "C\\vec{v_i} = (\\sigma^i)^2\\vec{v_i}\\;.\n",
    "$$\n",
    "\n",
    "Now, we shall prove that this distribution is normalized. The integral we need to evaluate is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}P(\\vec{x})d^n x = \\int_{-\\infty}^{\\infty} \\frac{\\exp\\left[{\\frac{- (\\vec{x} - \\vec{\\mu})^T C^{-1} (\\vec{x}-\\vec{\\mu})}{2}}\\right]}{\\left(\\sqrt{2\\pi}\\right)^n \\sqrt{det\\;C}} d^n x\\;.\n",
    "$$\n",
    "\n",
    "Let us call $\\vec{x}-\\vec{\\mu} = \\alpha^i\\vec{v_i}$, therefore the derivative on ints components is $\\frac{\\partial x^i}{\\partial \\alpha^j} = v^i_j$. From the *eigen-value* equation, we find that $C^{-1}(\\alpha^i\\vec{ v_i}) =\\sum_i \\frac{1}{(\\sigma^i)^2} (\\alpha^i \\vec{v_i})$. Furthermore, knowing that our basis $\\{v\\}$ is orthogonal means $\\vec{v_i} \\cdot \\vec{v_j} = \\delta_{ij}$, we can rewrite our integral as:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}P(\\vec{x})d^n x = \\int_{-\\infty}^{\\infty} \\frac{e^{-\\sum_i \\frac{(\\alpha^i)^2}{2(\\sigma^i)^2}}}{(\\sqrt{2\\pi})^n \\prod_i (\\sigma^i)} d^n \\alpha\\;.\n",
    "$$\n",
    "\n",
    "Where $\\prod_i (\\sigma^i) = \\sqrt{det\\;C}$, by diagonalizing the matrix in its *eigen-values*. One can see that for every $n$ possible index, $i$, we will find an integral with known value of $1$.\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^1)^2}{2(\\sigma^1)^2}}}{\\sqrt{2\\pi} (\\sigma^1)} d \\alpha^1 \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^2)^2}{2(\\sigma^2)^2}}}{\\sqrt{2\\pi} (\\sigma^2)} d \\alpha^2 \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^3)^2}{2(\\sigma^3)^2}}}{\\sqrt{2\\pi} (\\sigma^3)} d \\alpha^3 \\times ... \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^n)^2}{2(\\sigma^n)^2}}}{\\sqrt{2\\pi} (\\sigma^n)} d \\alpha^n = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801736fd-1433-4240-afe3-2304b47c3de7",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3617b5-c846-49de-9a18-f2e51234815c",
   "metadata": {},
   "source": [
    "Suppose we have a random variable $Z$ given by the sum of random variables $x_i$, where $x_i$ are not identically independently distributed (i.i.d). The expected value of $Z$ is\n",
    "$$\n",
    "\\langle Z\\rangle  = \\sum_{i=1}^N\\langle x_i\\rangle  = \\sum_{i=1}^N \\int d^n x P(x) x_i = \\sum_{i=1}^N \\mu_i\\;,\n",
    "$$\n",
    "where $\\mu_i$ is the mean value of $x_i$. The variance of $Z$ is\n",
    "$$\n",
    "V(Z) = \\langle(Z - \\langle Z \\rangle)^2\\rangle = \\bigg \\langle \\left[\\sum_{i=1}^N (x_i - \\mu_i)\\right]^2 \\bigg \\rangle = \\bigg \\langle \\sum_{i=1}^N (x_i - \\mu_i) \\sum_{j=1}^N (x_j - \\mu_j) \\bigg \\rangle\\;,\n",
    "$$\n",
    "which can be divided into two expected values, one was $i=j$ and one with $i\\ne j$. By doing this we find\n",
    "$$\n",
    "V(Z) = \\bigg \\langle \\sum_{i=1}^N (x_i - \\mu_i)^2 \\bigg \\rangle + \\bigg \\langle \\sum_{i \\ne j}^N (x_i - \\mu_i)(x_j - \\mu_j) \\bigg \\rangle = V_i + C_{ij}\\;,\n",
    "$$\n",
    "where $V_i$ is the variance of every $x_i$ and $C_{ij}$ is the estimator of the covariance.\n",
    "\n",
    "Reviewing our variable $Z$, we see that it has an expected value and variance given by the respective sum of its components, *i.e.*\n",
    "\n",
    "- $\\mu_Z = \\sum \\mu_i$;\n",
    "- $V(Z) = \\sum V_i$.\n",
    "\n",
    "This information tells us that these values shall asymptotically ascend to a Gaussian distribution as $N$ tends to $\\infty$.\n",
    "\n",
    "The **Central limit theorem** establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.[[Wiki]](https://en.wikipedia.org/wiki/Central_limit_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8416d8-66df-407c-a51d-c1baed2a9495",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8714f8-db6e-4c29-ad77-8fac7cdf9855",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aadb99-c441-4af1-a7b5-125b5b7b9239",
   "metadata": {},
   "source": [
    "Given a certain sample $\\{x_i\\}$ related to a particular distribution $P(\\{x_i\\})$, we say $f(\\{x_i\\})$ is a consistent estimator of $\\theta$ -being $\\theta$ a particular property of $P$- if $\\lim_{N \\to \\infty} f(\\{x_i\\})= \\theta$. Furthermore, $f(\\{x_i\\})$ will not be biased when $\\langle f(\\{x_i\\}) \\rangle = \\theta$. Finnally, $f(\\{x_i\\})$ will be efficient if its variance is sufficiently small. A function $f(\\{x_i\\})$ that respects these three caracteristics is called an estimator. **VAGUE ಠ_ಠ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb8d9d-acfb-43a6-9451-66e93390c4aa",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234ddde-4bd8-4728-bf3a-e05a6b1085ff",
   "metadata": {},
   "source": [
    "We already know the expected value of a distribution, $\\mu = \\langle x \\rangle = \\int x P(x) dx$. Based on the average value, we shall create a bunch of estimators.\n",
    "\n",
    "* $\\hat{\\mu}_1 = \\sum_{i=1}^N \\frac{x_i}{N}$: Consistent, unbiased, and efficienty;\n",
    "\n",
    "* $\\hat{\\mu}_2 = \\sum_{i=1}^{Min(N, 10)} \\frac{x_i}{Min(N, 10)}$: Unconsisten, unbiased, less efficient than $\\hat{\\mu}_1$ for $N>10$;\n",
    "\n",
    "* $\\hat{\\mu}_3 = \\sum_{i=1}^N \\frac{x_i}{N-1}$: Consistent, biased, slightly less efficient than $\\hat{\\mu}_1$;\n",
    "\n",
    "* $\\hat{\\mu}_4 = 1,8$: Consistent and unbiased if and only if $\\mu = 1,8$, but infinitedly efficient;\n",
    "\n",
    "* $\\hat{\\mu}_5 = \\left( \\prod_{i=1}^N x_i\\right)^\\frac{1}{N}$: Consistancy and efficiency depends on $P(x)$, bias depends on the expected value of $x$;\n",
    "\n",
    "* $\\hat{\\mu}_6 = x_p$, where $x_p$ is the most frequent value of $\\{x_i\\}$: Consistancy, bias and efficiency depends on $P(x)$;\n",
    "\n",
    "* $\\hat{\\mu}_7 = \\frac{Max(\\{x_i\\}) + Min(\\{x_i\\})}{2}$: Consistancy, bias and efficiency depends on $P(x)$;\n",
    "\n",
    "* $\\hat{\\mu}_8 = \\sum_{i=1}^{|\\underline{N/2}|} \\frac{x_{2i}}{|\\underline{N/2}|}$, where $|\\underline{N/2}| = \\frac{N}{2}$ for even $N$, and $|\\underline{N/2}| = \\frac{(N-1)}{2}$ for odd values: Consistent, unbiased, less efficient than $\\hat{\\mu}_1$.\n",
    "\n",
    "We fall on the assumption that the first estimator is the best, but that is not true for every sample. As an exercise, you should check the consistency, efficiency, and bias of every estimator above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620b93-c80a-403b-9be3-f4f0afba6f58",
   "metadata": {},
   "source": [
    "## Likelihood and Fisher's Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237e128-c4ef-4985-9a67-cd8a6a8a298d",
   "metadata": {},
   "source": [
    "The likelihood function (often simply called the likelihood) describes the joint probability of the observed data as a function of the parameters of the chosen statistical model. For each specific parameter value $\\theta$  in the parameter space, the likelihood function $P(X, \\theta)$ therefore assigns a probabilistic prediction to the observed data $X$. Mathematically, we say that given an hypothesis $H_0$, the likelihood is\n",
    "$$\n",
    "\\mathcal{L}\\left(\\{x_i\\}|H_0\\right) = \\prod_{i=1}^N P(x_i|\\theta)\\;.\n",
    "$$\n",
    "The hypothesis encapsulates the functional form of $P(x_i)$ and the set of parameters $\\{\\theta\\}$.\n",
    "\n",
    "In terms of the likelihood, the expected value of a statistics is given by\n",
    "$$\n",
    "\\langle f \\rangle = \\int d^n x f(\\{x_i\\}) \\mathcal{L}\\left(\\{x_i\\}|\\vec{\\theta}\\right)\\;.\n",
    "$$\n",
    "From now on, we will write $\\mathcal{L}\\left(\\{x_i\\}|\\vec{\\theta}\\right)$ as $\\mathcal{L}$. Now, say we have an unbiased estimator $\\hat{\\theta}(\\{x_i\\})$, then\n",
    "$$\n",
    "\\langle \\hat{\\theta} \\rangle = \\int d^n x \\hat{\\theta}(\\{x_i\\}) \\mathcal{L} = \\theta \\;,\n",
    "$$\n",
    "which is a simple function of $\\theta$. Therefore, we can differentiate the right equation. By doing this we find\n",
    "$$\n",
    "\\int d^n x \\hat{\\theta}(\\{x_i\\}) \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = 1\\;.\n",
    "$$\n",
    "Dividing both sides by $\\mathcal{L}$, and rewriting $\\frac{1}{\\mathcal{L}}\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ as $\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}$ we get\n",
    "$$\n",
    "\\int d^n x \\hat{\\theta}(\\{x_i\\}) \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} = 1\\;.\n",
    "$$\n",
    "\n",
    "Beyond that, we know that the distribution is normalizes, *i.e*,\n",
    "$$\n",
    "\\int d^n x \\mathcal{L} = 1\\;,\n",
    "$$\n",
    "were its derivative is\n",
    "$$\n",
    "\\int d^n x \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = 0\\; ;\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int  \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} d^n x = 0\\;.\n",
    "$$\n",
    "\n",
    "This last equation can be seen as the expected value of $\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}$, therefore, this expression is a statistic with null expected value,\n",
    "$$\n",
    "\\bigg \\langle \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\bigg \\rangle = 0\\;.\n",
    "$$\n",
    "\n",
    "Finnally, we subtract $\\theta$ times the null integral from the unitary integral, *i.e*\n",
    "$$\n",
    "\\int d^n x \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right] \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} = 1\\;.\n",
    "$$\n",
    "\n",
    "Before we go on, we must remind a little about the **Cauchy-Schwarz inequality**. We will use a possible generalization, when we define the norm of a function as\n",
    "$$\n",
    "\\int f^*f d^n x = \\int f^2 d^n x\\;.\n",
    "$$\n",
    "Then, the inequality becomes\n",
    "$$\n",
    "\\left(\\int f^2 d^n x\\right)\\left(\\int g^2 d^n x\\right)\\geq \\left(\\int fg d^n x\\right)^2\\;.\n",
    "$$\n",
    "\n",
    "If we set $f = \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right] \\sqrt{\\mathcal{L}}$, and $g = \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\sqrt{\\mathcal{L}}$, knowing that the integral of their product is unitary, we find\n",
    "$$\n",
    "\\left(\\int \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right]^2 \\mathcal{L} d^n x\\right)\\left(\\int \\left[\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right]^2 \\mathcal{L} d^n x\\right)\\geq 1\\;.\n",
    "$$\n",
    "We can identify the first integral as $\\langle \\left(\\hat{\\theta}(\\{x_i\\}) - \\theta \\right)^2 \\rangle = V(\\hat{\\theta})$, therefore,\n",
    "$$\n",
    "V(\\hat{\\theta}) \\geq \\frac{1}{\\int \\left[\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right]^2 \\mathcal{L} d^n x}\\;.\n",
    "$$\n",
    "\n",
    "The result we have found is of extreme importance! The variance of an unbiased estimator has a minimal value which depends only on our hypothesis about the distribution and its parameters. We have achieved this result without any data whatsoever. The denominator on the right side of the equation is called **Fisher Information**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d093f-32f5-48ec-86c8-206634a657b2",
   "metadata": {},
   "source": [
    "# Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce89d6-c546-472d-9085-fecd9cd14444",
   "metadata": {},
   "source": [
    "## Variance of a Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05acb1-ba0a-437f-98ca-7e59c2fef173",
   "metadata": {},
   "source": [
    "Say we have and identically independently distributed (i.i.d) set $\\{x_i\\}$, where all possible values comes from the same distribution $P(x)$. Then, we can say that $\\overline{x} = \\sum_i^N \\frac{x_i}{N}$ (arithmetic mean) is an estimator of the expected value , $\\langle x \\rangle = \\mu$, *i.e.*,\n",
    "$$\n",
    "\\hat{\\mu} = \\overline{x}\\;.\n",
    "$$\n",
    "\n",
    "We can now calculate the variance of the estimator, $V(\\hat{\\mu}) = \\langle \\hat{\\mu}^2 \\rangle - \\langle \\hat{\\mu} \\rangle^2$. First, we calculate $\\langle \\hat{\\mu}^2 \\rangle$:\n",
    "$$\n",
    "\\langle \\hat{\\mu}^2 \\rangle = \\bigg \\langle \\left(\\sum_i^N \\frac{x_i}{N} \\right)^2 \\bigg \\rangle = \\frac{1}{N}\\bigg \\langle \\sum_i^N  x_i^2 \\bigg \\rangle + \\frac{1}{N}\\bigg \\langle \\sum_{i \\ne j} x_i x_j \\bigg \\rangle\\;\\; \\bigg |\\;\\langle x_i^2\\rangle = V(x) + \\mu^2\\;,\\langle x_i x_j \\rangle = \\mu^2\\;;\n",
    "$$\n",
    "The sum over $i$ of a constant is $N$, and the sum over $i \\neq j$ of a constant is $N^2-N$, therefore the variance of our estimator will be\n",
    "$$\n",
    "V(\\hat{\\mu}) = \\frac{V(x)}{N}\\;.\n",
    "$$\n",
    "\n",
    "This is a significant value because it says the variance of our estimator is more narrow than the variance of our distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8f555-7372-46e4-90f4-085d33f2dc1a",
   "metadata": {},
   "source": [
    "### Unbiased Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819528c6-406b-470b-89e8-ff581a107540",
   "metadata": {},
   "source": [
    "As we defined at the beginning of the course, the variance of a sample is\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N} \\sum_{i=1}^N \\left(x_i - \\overline{x}\\right)^2\\;.\n",
    "$$\n",
    "Now, that we have knowledge of estimators such as $\\hat{\\mu} = \\overline{x}$, we can rewrite the expression above with this substitution, which gives us\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N} \\sum_{i=1}^N \\left(x_i - \\hat{\\mu}\\right)^2\\ = \\frac{1}{N} \\sum_{i=1}^N \\left( x_i^2 - 2x_i\\hat{\\mu} + 2\\hat{\\mu}^2 \\right)\\;.\n",
    "$$\n",
    "\n",
    "To validate the bias of $\\hat{V}(x)$, its expected value should be equal to $\\hat{V}(x)$. Therefore,\n",
    "$$\n",
    "\\big \\langle \\hat{V}(x) \\big \\rangle = \\frac{1}{N} \\left[\\sum_{i=1}^N \\big \\langle \\left( x_i^2 - 2x_i\\hat{\\mu} + 2\\hat{\\mu}^2 \\right)\\big \\rangle\\right] = \\frac{1}{N} \\left[\\sum_{i=1}^N \\langle x_i^2 \\rangle - \\langle 2 \\hat{\\mu} \\sum_{i=1}^N x_i \\rangle + \\langle \\sum_{i=1}^N 2\\hat{\\mu}^2 \\rangle  \\right]\\;.\n",
    "$$\n",
    "Replacing some known values from the last section we get\n",
    "$$\n",
    "\\big \\langle \\hat{V}(x) \\big \\rangle = \\frac{N-1}{N} V(x)\\;.\n",
    "$$\n",
    "\n",
    "The previously defined variance is biased! Nevertheless, it is easy to eliminate such bias. We can simply redefine the variance as\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(x_i - \\hat{\\mu}\\right)^2\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef744816-2979-4eab-9a3d-50cffb41bef5",
   "metadata": {},
   "source": [
    "### Variance of the Variance Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048e6c8-df4e-4dca-816c-debc33400c69",
   "metadata": {},
   "source": [
    "!!HERE IT WOULD BE COOL TO PLOT AN INTERACTIVE GRAPH WITH MULTIPLE SAMPLES OF N VALUES AND ITS MEAN VALUES AND VARIANCE!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9097cdb-4640-4df0-963c-c093d179a988",
   "metadata": {},
   "source": [
    "It is also possible to calculate the variance of the variance estimator. It will be\n",
    "$$\n",
    "V(\\hat{V}) = \\big \\langle \\left(\\hat{V} - \\langle \\hat{V} \\rangle \\right)^2\\big \\rangle = \\bigg \\langle \\left( \\frac{1}{N-1} \\right)^2 \\left( \\sum_i (x_i - \\hat{\\mu}) \\right)^2 \\bigg \\rangle = \\frac{\\sigma^4}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fe9f7-256d-45e9-99d0-19816a3cc71d",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db5aeb-dca4-4b37-b1ef-5a4c341ccc37",
   "metadata": {},
   "source": [
    "Suppose we have a accurate functional form of a distribution $P(x_i|\\theta)$, what should we expect from the likelihood $\\mathcal{L}(\\{x_i\\}|\\theta)$? Seems reasonable to think that a correct assumption will give us a maximal value of the likelihood function. \n",
    "\n",
    "**Maximum Likelihood Estimation** is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable [[Wiki]](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n",
    "\n",
    "We shall define a estimator $\\hat{\\theta} = Max\\; \\mathcal{L}(\\{x_i\\}|\\theta)$, simply calculated by\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\bigg |_{\\hat{\\theta}} = 0\\;.\n",
    "$$\n",
    "\n",
    "Furthermore, given the scale of number of which we are working with it is common practice to work with $\\chi^2 = -2 \\ln \\mathcal{L}$. Then, the above estimator becomes $\\hat{\\theta} = Min\\; (-2 \\ln \\mathcal{L})$.\n",
    "\n",
    "In case we have more than one dimension, the maximization -or minimization- must be absolute, *i.e.*, the extreme value of every parameter. In other words, the gradient of the likelihood should be null. Otherwise, you will need to formulate a hypothesis for sets of parameters, since it is clearly difficult to have complete knowledge about the entirety of the space.\n",
    "\n",
    "Additionally, say we have a likelihood of a sample $x_i$, given a set of parameters $\\vec{\\theta}$ and $\\vec{\\psi}$. These parameter are inversible functions of each other, *i.e.*, $\\vec{\\theta}(\\vec{\\psi})$ and $\\vec{\\psi}(\\vec{\\theta})$. If we try to extremize $\\chi^2$, it is easy to see that a estimator is independent of the parametrization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f16bb-cd88-403c-9b47-6434fb1c9cf1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb16691-ffa3-4601-8c83-1d3a661f5fa0",
   "metadata": {},
   "source": [
    "The likelihood of a gaussian distribution is\n",
    "$$\n",
    "\\mathcal{L}(\\{x_i\\}|\\mu_i\\{\\sigma_i\\}) = \\frac{\\prod e^{-\\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2}}}{(\\sqrt{2 \\pi})^2\\prod \\sigma_i}\\;,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(x_i - \\mu)^2}{\\sigma_i^2} + 2 \\sum \\ln \\sigma_i + \\frac{N}{2}\\ln 2\\pi\\;.\n",
    "$$\n",
    "\n",
    "Taking the derivative of $\\chi^2$ with respect to $\\mu$, calculating it in a random estimator $\\hat{\\mu}$, and setting it to zero to extremize, we get\n",
    "$$\n",
    "\\frac{\\partial \\chi^2}{\\partial \\mu} \\bigg |_{\\hat{\\mu}} = -2\\sum \\frac{(x_i - \\hat{\\mu})}{\\sigma_i^2}=0\\;.\n",
    "$$\n",
    "\n",
    "If we call $w_i = 1/\\sigma^2$ and $w = \\sum w_i$, the right equation becomes\n",
    "$$\n",
    "\\sum w_i x_i - \\hat{\\mu}\\sum w_i = 0\\;,\n",
    "$$\n",
    "which gives us the estimator with maximum likelihood,\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{w}\\sum_i w_i x_i\\;.\n",
    "$$\n",
    "This is the weighted average. Furthermore, if every $\\sigma_i = \\sigma$, then $w = N/\\sigma^2$, and\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_i x_i\\;,\n",
    "$$\n",
    "which tells us the arithmetic mean also is an estimator with maximum likelihood.\n",
    "\n",
    "On top of that, if we want to find a the concurrent estimator with maximum likelihood for $\\sigma$, we take the derivative with respect to $\\sigma$ instead of $\\mu$. Doing so, we find\n",
    "$$\n",
    "\\frac{\\partial \\chi^2}{\\partial \\sigma} \\bigg |_{\\hat{\\sigma}} = -\\frac{2}{\\hat{\\sigma}^3}\\sum (x_i - \\hat{\\mu})^2 + \\frac{2N}{\\hat{\\sigma}} = 0\\;;\n",
    "$$\n",
    "$$\n",
    "\\hat{\\sigma}^2 =\\sum  \\frac{(x_i - \\hat{\\mu})^2}{N}\\;.\n",
    "$$\n",
    "We see that $\\hat{\\sigma}^2$ is the estimator of variance with maximum likelihood. Notice that this estimator is biased. The reason for that is the freedom of parametrization we previously discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354e9d1-50ab-4d0b-a13c-6794d133e25d",
   "metadata": {},
   "source": [
    "### Supernovae Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f763779-61f5-45ee-b5db-2f44610d7e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
